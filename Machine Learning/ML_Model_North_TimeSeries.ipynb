{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c59552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import hvplot.pandas\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ebcaa",
   "metadata": {},
   "source": [
    " ## Read the CSV (From Database using SQL Alchemy)  and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL toolkit and Object Relational Mapper\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, func\n",
    "from config import db_password\n",
    "\n",
    "\n",
    "db_string = f\"postgresql://postgres:{db_password}@seaice.ck2g7em9g3ik.us-east-1.rds.amazonaws.com:5432/postgres\"\n",
    "engine = create_engine(db_string)\n",
    "\n",
    "North_df=pd.read_sql_table(\"North\", con=engine, parse_dates=['time'], index_col='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618df73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "North_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0cdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "\n",
    "North_df.drop(['index', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c21cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns\n",
    "North_df = North_df.rename(columns={'temperature_2m':'2m Temperature (K)', 'siconc':'Sea Ice Concentration(0 to 1)', 'Snow_albedo':'Snow Albedo (0 to 1)',\n",
    "                                    'snow_melt':'Snow Melt (m of water)', \n",
    "                                    'surface_pressure':'Surface Pressure (Pa)','Total_column_ozone': 'Total Column Ozone (kg m2)',\n",
    "                                    'xco2':'Average Atmospheric Carbon di oxide mixing ratio (ppm)',\n",
    "                                    'xch4':'Average Atmospheric Methane mixing ratio (ppm)', '     Extent':'Extent (sq km)'})\n",
    "North_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebcb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unique values in each column (if relevant to data)\n",
    "\n",
    "North_df.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8627767",
   "metadata": {},
   "outputs": [],
   "source": [
    "North_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c478c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time to date\n",
    "\n",
    "# North_df['time of day'] = pd.Series([\" 00:00\" for x in range(len(North_df.index))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# North_df[\"time\"] = North_df[\"time\"].astype(str) + North_df[\"time of day\"].astype(str)\n",
    "# North_df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# North_df.drop('time of day', axis=1, inplace=True)\n",
    "# North_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04dbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time to dt.date\n",
    "\n",
    "# from datetime import datetime\n",
    "\n",
    "# North_df['time']= pd.to_datetime(North_df['time'], format = '%Y-%m-%d %H:%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "North_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb767b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8714c247",
   "metadata": {},
   "source": [
    "## Evolution of features over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211bb97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize Time Series\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, dpi=120, figsize=(10,6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    data = North_df[North_df.columns[i]]\n",
    "    ax.plot(data, color='red', linewidth=1)\n",
    "    # Decorations\n",
    "    ax.set_title(North_df.columns[i])\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics of dataset\n",
    "\n",
    "North_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06100d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce2f6a",
   "metadata": {},
   "source": [
    "## Check correlation between features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ca18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(North_df['Extent (sq km)'])\n",
    "plt.title('Time vs Extent')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Extent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['2m Temperature (K)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['2m Temperature (K)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['2m Temperature (K)'], a*North_df['2m Temperature (K)']+b)\n",
    "plt.title('Temp vs Extent')\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('Extent')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4752ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['Sea Ice Concentration(0 to 1)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['Sea Ice Concentration(0 to 1)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['Sea Ice Concentration(0 to 1)'], a*North_df['Sea Ice Concentration(0 to 1)']+b)\n",
    "plt.title('Sea Ice Conentration vs Extent')\n",
    "plt.xlabel('Sea Ice Concentration')\n",
    "plt.ylabel('Extent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c523638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['Snow Albedo (0 to 1)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['Snow Albedo (0 to 1)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['Snow Albedo (0 to 1)'], a*North_df['Snow Albedo (0 to 1)']+b)\n",
    "plt.title('Snow Albedo vs Extent')\n",
    "plt.xlabel('Snow Albedo')\n",
    "plt.ylabel('Extent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['Snow Melt (m of water)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['Snow Melt (m of water)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['Snow Melt (m of water)'], a*North_df['Snow Melt (m of water)']+b)\n",
    "plt.title('Snow Melt vs Extent')\n",
    "plt.xlabel('Snow Melt')\n",
    "plt.ylabel('Extent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['Total Column Ozone (kg m2)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['Total Column Ozone (kg m2)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['Total Column Ozone (kg m2)'], a*North_df['Total Column Ozone (kg m2)']+b)\n",
    "plt.title('Surface Pressure vs Extent')\n",
    "plt.xlabel('Surface Pressure')\n",
    "plt.ylabel('Extent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae16ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['Surface Pressure (Pa)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['Surface Pressure (Pa)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['Surface Pressure (Pa)'], a*North_df['Surface Pressure (Pa)']+b)\n",
    "plt.title('Ozone vs Extent')\n",
    "plt.xlabel('Ozone')\n",
    "plt.ylabel('Extent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0312f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['Average Atmospheric Carbon di oxide mixing ratio (ppm)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['Average Atmospheric Carbon di oxide mixing ratio (ppm)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['Average Atmospheric Carbon di oxide mixing ratio (ppm)'], a*North_df['Average Atmospheric Carbon di oxide mixing ratio (ppm)']+b)\n",
    "\n",
    "plt.title('Co2 vs Extent')\n",
    "plt.xlabel('Co2 Emissiton')\n",
    "plt.ylabel('Extent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find line of best fit\n",
    "a, b = np.polyfit(North_df['Average Atmospheric Methane mixing ratio (ppm)'],North_df['Extent (sq km)'], 1)\n",
    "\n",
    "#add points to plot\n",
    "plt.plot(North_df['Average Atmospheric Methane mixing ratio (ppm)'],North_df['Extent (sq km)'])\n",
    "\n",
    "#add line of best fit to plot\n",
    "plt.plot(North_df['Average Atmospheric Methane mixing ratio (ppm)'], a*North_df['Average Atmospheric Methane mixing ratio (ppm)']+b)\n",
    "\n",
    "plt.title('Cox4 vs Extent')\n",
    "plt.xlabel('Cox4 Emission')\n",
    "plt.ylabel('Extent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f3e39",
   "metadata": {},
   "source": [
    "## METHOD 1: Vector Autoregression (VAR)\n",
    "\n",
    "## Causality Testing\n",
    "\n",
    "First, we use Granger Causality Test to investigate causality of data. Granger causality is a way to investigate the causality between two variables in a time series which actually means if a particular variable comes before another in the time series. In the MTS, we will test the causality of all combinations of pairs of variables.\n",
    "\n",
    "The Null Hypothesis of the Granger Causality Test is that lagged x-values do not explain the variation in y, so the x does not cause y. We use grangercausalitytests function in the package statsmodels to do the test and the output of the matrix is the minimum p-value when computes the test for all lags up to maxlag. The critical value we use is 5% and if the p-value of a pair of variables is smaller than 0.05, we could say with 95% confidence that a predictor x causes a response y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc0f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "variables=North_df.columns  \n",
    "\n",
    "matrix = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "   \n",
    "for col in matrix.columns:\n",
    "    for row in matrix.index:\n",
    "        test_result = grangercausalitytests(North_df[[row, col]], len(variables), verbose=False)            \n",
    "        p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(len(variables))]            \n",
    "        min_p_value = np.min(p_values)\n",
    "        matrix.loc[row, col] = min_p_value\n",
    "matrix.columns = [var + '_x' for var in variables]\n",
    "matrix.index = [var + '_y' for var in variables]\n",
    "\n",
    "print(matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48477d16",
   "metadata": {},
   "source": [
    "## Stationary Test\n",
    "As VectorARIMA requires time series to be stationary, we will use one popular statistical test – Augmented Dickey-Fuller Test (ADF Test) to check the stationary of each variable in the dataset. If the stationarity is not achieved, we need to make the data stationary, such as eliminating the trend and seasonality by differencing and seasonal decomposition.\n",
    "\n",
    "In the following script, we use adfuller function in the statsmodels package for stationary test of each variables. The Null Hypothesis is that the data has unit root and is not stationary and the significant value is 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adfuller_test(series, sig=0.05, name=''):\n",
    "    res = adfuller(series, autolag='AIC')    \n",
    "    p_value = round(res[1], 3) \n",
    "\n",
    "    if p_value <= sig:\n",
    "        print(f\" {name} : P-Value = {p_value} => Stationary. \")\n",
    "    else:\n",
    "        print(f\" {name} : P-Value = {p_value} => Non-stationary.\")\n",
    "\n",
    "for name, column in North_df.iteritems():\n",
    "    adfuller_test(column, name=column.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0da554",
   "metadata": {},
   "outputs": [],
   "source": [
    "North_df_differenced = North_df.diff().dropna()\n",
    "for name, column in North_df_differenced.iteritems():\n",
    "    adfuller_test(column, name=column.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea05d68",
   "metadata": {},
   "source": [
    "# Co-integration Test\n",
    "\n",
    "Cointegration test helps to establish the presence of a statistically significant connection between two or more time series. But, what does Cointegration mean? To understand that, you first need to know what is ‘order of integration’ (d). Order of integration(d) is nothing but the number of differencing required to make a non-stationary time series stationary. Now, when you have two or more time series, and there exists a linear combination of them that has an order of integration (d) less than that of the individual series, then the collection of series is said to be cointegrated. Ok? When two or more time series are cointegrated, it means they have a long run, statistically significant relationship. This is the basic premise on which Vector Autoregression(VAR) models is based on. So, it’s fairly common to implement the cointegration test before starting to build VAR models. Alright, So how to do this test? Soren Johanssen in his paper (1991) devised a procedure to implement the cointegration test. It is fairly straightforward to implement in python’s statsmodels, as you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree of differencing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e49831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(North_df, alpha=0.05): \n",
    "    \n",
    "    out = coint_johansen(North_df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Summary\n",
    "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(North_df.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(North_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ce871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "\n",
    "\n",
    "X= North_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6058c7f",
   "metadata": {},
   "source": [
    "## SPLIT THE DATA\n",
    "Splitting the dataset into training and test data. The VAR model will be fitted on df_train and then used to forecast the next 4 observations. These forecasts will be compared against the actuals present in test data. To do the comparisons, we will use multiple forecast accuracy metrics, as seen later in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "\n",
    "# X_train.head()\n",
    "\n",
    "# nobs is number of next observations\n",
    "nobs = 4\n",
    "North_df_train, North_df_test = North_df[0:-nobs], North_df[-nobs:]\n",
    "\n",
    "# Check size\n",
    "print(North_df_train.shape) \n",
    "print(North_df_test.shape)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a3c2d",
   "metadata": {},
   "source": [
    "# Grid Search for Order P\n",
    "\n",
    "To select the right order of the VAR model, we iteratively fit increasing orders of VAR model and pick the order that gives a model with least AIC. Though the usual practice is to look at the AIC, you can also check other best fit comparison estimates of BIC, FPE and HQIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db441ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "\n",
    "for i in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    model = VAR(North_df_differenced)\n",
    "    results = model.fit(i)\n",
    "\n",
    "    print('Order =', i)\n",
    "    print('AIC: ', results.aic)\n",
    "    print('BIC: ', results.bic)\n",
    "    print('FPE : ', results.fpe)\n",
    "    print('HQIC: ', results.hqic, '\\n')\n",
    "    print()\n",
    "\n",
    "# x = model.select_order(maxlags=15)\n",
    "# x.summary()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dee79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitted = model.fit(1)\n",
    "model_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a362ec2",
   "metadata": {},
   "source": [
    "## Check for Serial Correlation of Residuals (Errors) using Durbin Watson Statistic\n",
    "\n",
    "Serial correlation of residuals is used to check if there is any leftover pattern in the residuals (errors). W/ If there is any correlation left in the residuals, then, there is some pattern in the time series that is still left to be explained by the model. In that case, the typical course of action is to either increase the order of the model or induce more predictors into the system or look for a different algorithm to model the time series. So, checking for serial correlation is to ensure that the model is sufficiently able to explain the variances and patterns in the time series. A common way of checking for serial correlation of errors can be measured using the Durbin Watson’s Statistic.Durbin Watson Statistic - FormulaThe value of this statistic can vary between 0 and 4. The closer it is to the value 2, then there is no significant serial correlation. The closer to 0, there is a positive serial correlation, and the closer it is to 4 implies negative serial correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1918fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(model_fitted.resid)\n",
    "\n",
    "for col, val in zip(North_df.columns, out):\n",
    "    print(col, ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99409246",
   "metadata": {},
   "source": [
    "## Forecast VAR model using statsmodels\n",
    "In order to forecast, the VAR model expects up to the lag order number of observations from the past data. This is because, the terms in the VAR model are essentially the lags of the various time series in the dataset, so you need to provide it as many of the previous values as indicated by the lag order used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lag order\n",
    "lag_order = model_fitted.k_ar\n",
    "print(lag_order) \n",
    "\n",
    "# Input data for forecasting\n",
    "forecast_input = North_df_differenced.values[-lag_order:]\n",
    "forecast_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "fc = model_fitted.forecast(y=forecast_input, steps=nobs)\n",
    "North_df_forecast = pd.DataFrame(fc, index=North_df.index[-nobs:], columns=North_df.columns + '_2d')\n",
    "North_df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaea83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_transformation(North_df_train, North_df_forecast, second_diff=False):\n",
    "    # \"\"\"Revert back the differencing to get the forecast to original scale.\"\"\"\n",
    "    North_df_fc = North_df_forecast.copy()\n",
    "    columns = North_df_train.columns\n",
    "    for col in columns:        \n",
    "        # Roll back 2nd Diff\n",
    "        if second_diff:\n",
    "            North_df_fc[str(col)+'_1d'] = (North_df_train[col].iloc[-1]-North_df_train[col].iloc[-2]) + North_df_fc[str(col)+'_2d'].cumsum()\n",
    "        # Roll back 1st Diff\n",
    "            North_df_fc[str(col)+'_forecast'] = North_df_train[col].iloc[-1] + North_df_fc[str(col)+'_1d'].cumsum()\n",
    "    return North_df_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "North_df_results = invert_transformation(North_df_train, North_df_forecast, second_diff=False)        \n",
    "\n",
    "North_df_results.loc[:, ['2m Temperature (K)_2d', 'Sea Ice Concentration(0 to 1)_2d', 'Snow Albedo (0 to 1)_2d',\n",
    "                         'Snow Melt (m of water)_2d', 'Surface Pressure (Pa)_2d','Total Column Ozone (kg m2)_2d',\n",
    "                   'Average Atmospheric Carbon di oxide mixing ratio (ppm)_2d',\n",
    "                   'Average Atmospheric Methane mixing ratio (ppm)_2d','Extent (sq km)_2d']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6468caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=int(len(North_df.columns)/2), ncols=2, dpi=150, figsize=(10,10))\n",
    "for i, (col,ax) in enumerate(zip(North_df.columns, axes.flatten())):\n",
    "    North_df_results[col+'_2d'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    North_df_test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
    "    me = np.mean(forecast - actual)             # ME\n",
    "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
    "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    mins = np.amin(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    maxs = np.amax(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})\n",
    "\n",
    "print('Forecast Accuracy of: 2m Temperature')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['2m Temperature (K)_2d'], North_df_test['2m Temperature (K)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "\n",
    "print('Forecast Accuracy of: Sea Ice Concentration(0 to 1)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Sea Ice Concentration(0 to 1)_2d'], North_df_test['Sea Ice Concentration(0 to 1)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "    \n",
    "print('Forecast Accuracy of: Snow Albedo (0 to 1)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Snow Albedo (0 to 1)_2d'], North_df_test[ 'Snow Albedo (0 to 1)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "    \n",
    "print('Forecast Accuracy of: Snow Melt (m of water)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Snow Melt (m of water)_2d'], North_df_test['Snow Melt (m of water)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "    \n",
    "print('Forecast Accuracy of: Surface Pressure (Pa)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Surface Pressure (Pa)_2d'], North_df_test['Surface Pressure (Pa)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "    \n",
    "print('Forecast Accuracy of: Total Column Ozone (kg m2)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Total Column Ozone (kg m2)_2d'], North_df_test['Total Column Ozone (kg m2)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "    \n",
    "print('Forecast Accuracy of: Average Atmospheric Carbon di oxide mixing ratio (ppm)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Average Atmospheric Carbon di oxide mixing ratio (ppm)_2d'], North_df_test['Average Atmospheric Carbon di oxide mixing ratio (ppm)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "    \n",
    "print('Forecast Accuracy of: Average Atmospheric Methane mixing ratio (ppm)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Average Atmospheric Methane mixing ratio (ppm)_2d'], North_df_test['Average Atmospheric Methane mixing ratio (ppm)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))\n",
    "            \n",
    "print('Forecast Accuracy of: Extent (sq km)')\n",
    "accuracy_prod = forecast_accuracy(North_df_results['Extent (sq km)_2d'], North_df_test['Extent (sq km)'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print((k), ': ', round(v,4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c9418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8441b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365cd748",
   "metadata": {},
   "source": [
    "## Train & Evaluate Using Time Series ML Model\n",
    "\n",
    "use a (70%, 20%, 10%) split for the training, validation, and test sets. Note the data is not being randomly shuffled before splitting. This is for two reasons:\n",
    "\n",
    "It ensures that chopping the data into windows of consecutive samples is still possible.\n",
    "It ensures that the validation/test results are more realistic, being evaluated on the data collected after the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea52c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split our preprocessed data into our features and target arrays\n",
    "# X= North_df.drop([\"Extent (sq km)\"], axis=1)\n",
    "# y= North_df[\"Extent (sq km)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the preprocessed data into a training and testing dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac77398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3264846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for linear regression\n",
    "\n",
    "# cdf = North_df.copy()\n",
    "# mod = sm.OLS(y_train, X_train)\n",
    "\n",
    "# res = mod.fit()\n",
    "# print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2657955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create StandardScaler instances\n",
    "# scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the StandardScaler\n",
    "\n",
    "# X_scaler = scaler.fit(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the Data\n",
    "\n",
    "# X_train_scaled = X_scaler.transform(X_train)\n",
    "# X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608183e",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "It is important to scale features before training a neural network. Normalization is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(North_df.columns)}\n",
    "\n",
    "n = len(North_df)\n",
    "train_df = North_df[0:int(n*0.7)]\n",
    "val_df = North_df[int(n*0.7):int(n*0.9)]\n",
    "test_df = North_df[int(n*0.9):]\n",
    "\n",
    "num_features = North_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = (North_df - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(North_df.keys(), rotation=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722efa71",
   "metadata": {},
   "source": [
    "## Data windowing\n",
    "The models will make a set of predictions based on a window of consecutive samples from the data.\n",
    "\n",
    "The main features of the input windows are:\n",
    "\n",
    "The width (number of time steps) of the input and label windows.\n",
    "The time offset between them.\n",
    "Which features are used as inputs, labels, or both.\n",
    "\n",
    "Define a WindowGenerator class. This class can:\n",
    "\n",
    "Handle the indexes and offsets as shown in the diagrams above.\n",
    "Split windows of features into (features, labels) pairs.\n",
    "Plot the content of the resulting windows.\n",
    "Efficiently generate batches of these windows from the training, evaluation, and test data, using tf.data.Datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One month into future, given 3 past months\n",
    "\n",
    "w1 = WindowGenerator(input_width=3, label_width=1, shift=1,\n",
    "                     label_columns=['2m Temperature (K)'])\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae5bdb",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Given a list of consecutive inputs, the `split_window` method will convert them to a window of inputs and a window of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e28297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack three slices, the length of the total window.\n",
    "example_window = tf.stack([np.array(train_df[:w1.total_window_size]),\n",
    "                           np.array(train_df[100:100+w2.total_window_size]),\n",
    "                           np.array(train_df[200:200+w2.total_window_size])])\n",
    "\n",
    "example_inputs, example_labels = w1.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'Labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ab997",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468520c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f1ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b897e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bbaf677",
   "metadata": {},
   "source": [
    "## STEP 3: Compile, Train & Evaluate the Model (Using Tensor Flow) in Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "# Add layers (input, hidden, output)\n",
    "# Check model structure\n",
    "\n",
    "\n",
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "#  YOUR CODE GOES HERE\n",
    "\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 100\n",
    "hidden_nodes_layer2 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "#  YOUR CODE GOES HERE\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"linear\"))\n",
    "\n",
    "# Second hidden layer\n",
    "#  YOUR CODE GOES HERE\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"linear\"))\n",
    "\n",
    "# Output layer\n",
    "#  YOUR CODE GOES HERE\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b37bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile (add checkpoints, create callback to save weights)\n",
    "\n",
    "# import os\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Define the checkpoint path and filenames\n",
    "# # os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "# # checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\"\n",
    "\n",
    "nn.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback which saves the weights for every 5 epochs\n",
    "# cp_callback= ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True, save_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=1000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model – drop noisy variables, change layer, neurons, epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd48afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to HDF5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
